import logging
import asyncio
import re
from typing import List, Dict, Any, Tuple
from src.db.connection import DBConnection
from src.config.settings import settings
from src.utils.cleaning import normalize_numeric_string
from src.etl.cdc_processor import compute_row_hash, CDCProcessor

log = logging.getLogger('loader')

class DataLoader:
    def __init__(self):
        self.schema_prefix = 'staging.' if settings.use_staging_schema else ''
        # –†–∞–∑—Ä–µ—à–∞–µ–º –±—É–∫–≤—ã, —Ü–∏—Ñ—Ä—ã, –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–Ω–∏–µ –∏ —Ç–æ—á–∫–∏ (–¥–ª—è schema.table)
        self._ident_pattern = re.compile(r'^[a-zA-Z0-9_.]+$')

    def _validate_identifier(self, ident: str) -> str:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä (—Ç–∞–±–ª–∏—Ü–∞/–∫–æ–ª–æ–Ω–∫–∞) –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∏–Ω—ä–µ–∫—Ü–∏–π."""
        if not ident:
            raise ValueError("–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        if not self._ident_pattern.match(ident):
            raise ValueError(f"–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä: {ident}")
        return ident

    def _prepare_row(self, r: List[Any], col_names: List[str], row_num: int) -> Tuple[List[str], str]:
        """–£–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Å—Ç—Ä–æ–∫–∏: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ, –æ—á–∏—Å—Ç–∫–∞, —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ."""
        # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –∏ –ø—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å—Ç—Ä–æ–∫–µ
        full_row = list(r) + [None] * (len(col_names) - len(r))
        full_row = full_row[:len(col_names)]
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö (–≤—Å—ë –≤ —Å—Ç—Ä–æ–∫–∏ –¥–ª—è —Ö–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è)
        full_row_str = [normalize_numeric_string(val) for val in full_row]
        row_hash = compute_row_hash(full_row_str)
        
        return full_row_str, row_hash

    async def load_full_refresh(self, table: str, col_names: List[str], rows: List[List[Any]]) -> Dict[str, int]:
        """–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ —Ç–∞–±–ª–∏—Ü—ã: TRUNCATE + INSERT."""
        table = self._validate_identifier(table)
        validated_cols = [self._validate_identifier(c) for c in col_names]
        
        log.info(f"–ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–π –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∏ {table} ({len(rows)} —Å—Ç—Ä–æ–∫)")
        stats = {'inserted': 0, 'errors': 0}
        
        async with await DBConnection.get_connection() as conn:
            async with conn.transaction():
                await conn.execute(f'TRUNCATE TABLE {self.schema_prefix}"{table}"')
                
                prepared_records = []
                target_cols = validated_cols + ["_row_index", "__row_hash"]
                
                for idx, r in enumerate(rows):
                    row_num = idx + 2
                    try:
                        full_row_str, row_hash = self._prepare_row(r, col_names, row_num)
                        prepared_records.append(tuple(full_row_str + [row_num, row_hash]))
                    except Exception as e:
                        log.warning(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ —Å—Ç—Ä–æ–∫–∏ {row_num}: {e}")
                        stats['errors'] += 1
                
                if prepared_records:
                    target_schema = self.schema_prefix.replace('.', '') if self.schema_prefix else None
                    await conn.copy_records_to_table(
                        table,
                        schema_name=target_schema,
                        records=prepared_records,
                        columns=target_cols
                    )
                    stats['inserted'] = len(prepared_records)
                    
        log.info(f"–ü–æ–ª–Ω–∞—è –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∫–∞ {table} –∑–∞–≤–µ—Ä—à–µ–Ω–∞: {stats}")
        return stats

    async def load_cdc(self, table: str, col_names: List[str], rows: List[List[Any]], pk_field: str = '__row_hash') -> Dict[str, int]:
        """–ò–Ω–∫—Ä–µ–º–µ–Ω—Ç–∞–ª—å–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º CDC."""
        table = self._validate_identifier(table)
        pk_field = self._validate_identifier(pk_field)
        
        log.info(f"CDC –∑–∞–≥—Ä—É–∑–∫–∞ –≤ {table} ({len(rows)} —Å—Ç—Ä–æ–∫ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞) [PK: {pk_field}]")
        
        existing_hashes = await self._fetch_existing_hashes(table, pk_field)
        processor = CDCProcessor(existing_hashes)
        
        for idx, r in enumerate(rows):
            row_num = idx + 2
            try:
                full_row_str, row_hash = self._prepare_row(r, col_names, row_num)
                
                # PK identification
                if pk_field == '__row_hash':
                    pk_val = row_hash
                elif pk_field in col_names:
                    pk_idx = col_names.index(pk_field)
                    pk_val = full_row_str[pk_idx]
                else:
                    pk_val = None

                if not pk_val:
                     continue

                row_data = {col: val for col, val in zip(col_names, full_row_str)}
                row_data['_row_index'] = row_num
                
                processor.process_row(pk_val, row_hash, row_data)
            except Exception as e:
                log.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {row_num} –¥–ª—è CDC: {e}")

        processor.finalize()
        cdc_stats = processor.get_stats()
        await self._apply_cdc_changes(table, processor, col_names, pk_field)
        return cdc_stats

    async def calculate_changes(self, table: str, col_names: List[str], rows: List[List[Any]], pk_field: str = '__row_hash') -> Dict[str, int]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏–∑–º–µ–Ω–µ–Ω–∏–π –±–µ–∑ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è (–¥–ª—è dry-run)."""
        table = self._validate_identifier(table)
        pk_field = self._validate_identifier(pk_field)
        
        log.info(f"üîç [DRY-RUN] –†–∞—Å—á–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –¥–ª—è {table} [PK: {pk_field}]")
        
        existing_hashes = await self._fetch_existing_hashes(table, pk_field)
        processor = CDCProcessor(existing_hashes)
        
        for idx, r in enumerate(rows):
            row_num = idx + 2
            try:
                full_row_str, row_hash = self._prepare_row(r, col_names, row_num)
                
                if pk_field == '__row_hash':
                    pk_val = row_hash
                elif pk_field in col_names:
                    pk_idx = col_names.index(pk_field)
                    pk_val = full_row_str[pk_idx]
                else:
                    pk_val = None
                
                if not pk_val:
                    continue

                row_data = {col: val for col, val in zip(col_names, full_row_str)}
                processor.process_row(pk_val, row_hash, row_data)
            except Exception as e:
                log.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {row_num} (dry-run): {e}")

        processor.finalize()
        return processor.get_stats()

    async def _fetch_existing_hashes(self, table: str, pk_field: str) -> Dict[str, str]:
        # table –∏ pk_field —É–∂–µ –≤–∞–ª–∏–¥–∏—Ä–æ–≤–∞–Ω—ã –≤—ã—à–µ
        try:
            query = f'SELECT "{pk_field}" as pk, __row_hash FROM {self.schema_prefix}"{table}" WHERE "{pk_field}" IS NOT NULL'
            rows = await DBConnection.fetch(query)
            return {str(row['pk']): row['__row_hash'] for row in rows if row['__row_hash']}
        except Exception as e:
            log.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ö–µ—à–∏ –¥–ª—è {table} (–∫–æ–ª–æ–Ω–∫–∞ {pk_field} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç?): {e}")
            return {}

    async def _apply_cdc_changes(self, table: str, processor: CDCProcessor, col_names: List[str], pk_field: str):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç INSERT/UPDATE/DELETE –∑–∞–ø—Ä–æ—Å—ã."""
        table = self._validate_identifier(table)
        validated_cols = [self._validate_identifier(c) for c in col_names]
        pk_field = self._validate_identifier(pk_field)

        async with await DBConnection.get_connection() as conn:
            # INSERTs
            if processor.to_insert:
                cols_str = ', '.join([f'"{c}"' for c in validated_cols] + ['"_row_index"', '"__row_hash"'])
                placeholders = ', '.join([f'${i+1}' for i in range(len(validated_cols) + 2)])
                insert_query = f'INSERT INTO {self.schema_prefix}"{table}" ({cols_str}) VALUES ({placeholders})'
                
                for item in processor.to_insert:
                    data = item['data']
                    values = [data.get(c) for c in col_names] + [data.get('_row_index'), item['hash']]
                    await conn.execute(insert_query, *values)

            # UPDATEs
            if processor.to_update:
                for item in processor.to_update:
                    data = item['data']
                    pk_val = item['legacy_id']
                    
                    set_parts = []
                    vals = []
                    idx = 1
                    for c in col_names:
                        if c == pk_field: continue
                        set_parts.append(f'"{c}" = ${idx}')
                        vals.append(data.get(c))
                        idx += 1
                    
                    set_parts.append(f'"__row_hash" = ${idx}')
                    vals.append(item['hash'])
                    idx += 1
                    vals.append(pk_val) # PK for WHERE
                    
                    query = f'UPDATE {self.schema_prefix}"{table}" SET {", ".join(set_parts)} WHERE "{pk_field}" = ${idx}'
                    await conn.execute(query, *vals)

            # DELETEs
            if processor.to_delete:
                del_query = f'DELETE FROM {self.schema_prefix}"{table}" WHERE "{pk_field}" = $1'
                for pk_val in processor.to_delete:
                    await conn.execute(del_query, pk_val)
                log.info(f"–£–¥–∞–ª–µ–Ω–æ {len(processor.to_delete)} —Å—Ç—Ä–æ–∫ –∏–∑ {table}")
