#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä–∞—è –º–∏–≥—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ Google Sheets –≤ Supabase.
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Atomic Swap –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ–π –∑–∞–º–µ–Ω—ã –¥–∞–Ω–Ω—ã—Ö.
"""
import sys
import argparse
import asyncio
import logging
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent))

from src.utils.logger import setup_logger
from src.config.loader import load_sources_config
from src.etl.extractor import GSheetsExtractor
from src.db.connection import DBConnection

log = setup_logger()

async def get_table_count(schema: str, table: str) -> int:
    """–ü–æ–ª—É—á–∞–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ."""
    async with await DBConnection.get_connection() as conn:
        try:
            return await conn.fetchval(f'SELECT COUNT(*) FROM "{schema}"."{table}"')
        except Exception:
            return 0

async def check_duplicates(schema: str, table: str, pk_col: str = 'record_id') -> list:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ PK –≤–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ."""
    query = f"""
        SELECT "{pk_col}", COUNT(*) as cnt 
        FROM "{schema}"."{table}" 
        GROUP BY "{pk_col}" 
        HAVING COUNT(*) > 1
        LIMIT 10
    """
    async with await DBConnection.get_connection() as conn:
        return await conn.fetch(query)

async def check_hash_duplicates(schema: str, table: str) -> int:
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –ø–æ content_hash."""
    query = f"""
        SELECT COUNT(*) FROM (
            SELECT content_hash FROM "{schema}"."{table}" 
            GROUP BY content_hash HAVING COUNT(*) > 1
        ) sub
    """
    async with await DBConnection.get_connection() as conn:
        try:
            return await conn.fetchval(query) or 0
        except Exception:
            return 0

async def migrate_table(sheet_config: dict, extractor: GSheetsExtractor, force: bool = False):
    """–ú–∏–≥—Ä–∞—Ü–∏—è –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å Atomic Swap."""
    target = sheet_config['target_table']
    schema, table = target.split('.') if '.' in target else ('public', target)
    
    log.info(f"üöÄ –ù–∞—á–∞–ª–æ –º–∏–≥—Ä–∞—Ü–∏–∏: {target}")
    
    # 1. –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    spreadsheet_id = sheet_config['spreadsheet_id']
    gid = sheet_config.get('gid')
    range_name = sheet_config.get('range', 'auto')
    
    log.info(f"   –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ GSheets (gid={gid})...")
    rows, headers = await extractor.extract_sheet_data(spreadsheet_id, gid, range_name)
    source_count = len(rows)
    log.info(f"   –ü–æ–ª—É—á–µ–Ω–æ {source_count} —Å—Ç—Ä–æ–∫")
    
    if source_count == 0:
        log.error(f"‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∏–≥—Ä–∞—Ü–∏–∏ {target}")
        return False
    
    # 2. Pre-flight: —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä—ã
    current_count = await get_table_count(schema, table)
    if current_count > 0 and source_count < current_count * 0.8:
        if not force:
            log.error(f"‚ùå –í–ù–ò–ú–ê–ù–ò–ï: –ù–æ–≤—ã—Ö —Å—Ç—Ä–æ–∫ ({source_count}) –º–µ–Ω—å—à–µ 80% –æ—Ç —Ç–µ–∫—É—â–∏—Ö ({current_count}). –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ --force")
            return False
        log.warning(f"‚ö†Ô∏è –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å --force: {source_count} vs {current_count}")
    
    # 3. –°–æ–∑–¥–∞—ë–º –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É –∏ –∑–∞–≥—Ä—É–∂–∞–µ–º
    temp_table = f"{table}_new"
    backup_table = f"{table}_backup"
    
    async with await DBConnection.get_connection() as conn:
        # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ temp/backup –µ—Å–ª–∏ –µ—Å—Ç—å
        await conn.execute(f'DROP TABLE IF EXISTS "{schema}"."{temp_table}"')
        await conn.execute(f'DROP TABLE IF EXISTS "{schema}"."{backup_table}"')
        
        # –°–æ–∑–¥–∞—ë–º temp —Ç–∞–±–ª–∏—Ü—É –∫–∞–∫ –∫–æ–ø–∏—é —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        await conn.execute(f'CREATE TABLE "{schema}"."{temp_table}" (LIKE "{schema}"."{table}" INCLUDING ALL)')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ COPY
        log.info(f"   –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é —Ç–∞–±–ª–∏—Ü—É {temp_table}...")
        col_names = [h.lower().replace(' ', '_') for h in headers]
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π –¥–ª—è COPY
        records = []
        for i, row in enumerate(rows):
            # –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É
            if len(row) < len(col_names):
                row = row + [None] * (len(col_names) - len(row))
            records.append(tuple(row[:len(col_names)]))
        
        await conn.copy_records_to_table(
            temp_table,
            records=records,
            columns=col_names,
            schema_name=schema
        )
        log.info(f"   ‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(records)} –∑–∞–ø–∏—Å–µ–π")
    
    # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ record_id
    pk_col = sheet_config.get('pk', 'record_id')
    duplicates = await check_duplicates(schema, temp_table, pk_col)
    if duplicates:
        log.error(f"‚ùå –ù–∞–π–¥–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã {pk_col}: {[d[pk_col] for d in duplicates[:5]]}")
        async with await DBConnection.get_connection() as conn:
            await conn.execute(f'DROP TABLE "{schema}"."{temp_table}"')
        return False
    
    # 5. –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ hash (–ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ)
    hash_dups = await check_hash_duplicates(schema, temp_table)
    if hash_dups > 0:
        log.warning(f"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω–æ {hash_dups} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ content_hash (–≤–æ–∑–º–æ–∂–Ω–æ, —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ)")
    
    # 6. Atomic Swap
    log.info(f"   –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ atomic swap...")
    async with await DBConnection.get_connection() as conn:
        async with conn.transaction():
            # Rename old -> backup
            await conn.execute(f'ALTER TABLE "{schema}"."{table}" RENAME TO "{backup_table}"')
            # Rename new -> current
            await conn.execute(f'ALTER TABLE "{schema}"."{temp_table}" RENAME TO "{table}"')
    
    log.info(f"‚úÖ –ú–∏–≥—Ä–∞—Ü–∏—è {target} –∑–∞–≤–µ—Ä—à–µ–Ω–∞! ({source_count} —Å—Ç—Ä–æ–∫)")
    log.info(f"   –ë—ç–∫–∞–ø —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {schema}.{backup_table}")
    return True

async def main():
    parser = argparse.ArgumentParser(description='–ë—ã—Å—Ç—Ä–∞—è –º–∏–≥—Ä–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö')
    parser.add_argument('--sheets', required=True, help='–°–ø–∏—Å–æ–∫ –ª–∏—Å—Ç–æ–≤ —á–µ—Ä–µ–∑ –∑–∞–ø—è—Ç—É—é (–Ω–∞–ø—Ä–∏–º–µ—Ä: sales_hst,clients_hst)')
    parser.add_argument('--confirm', action='store_true', help='–ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ –æ–ø–µ—Ä–∞—Ü–∏–∏')
    parser.add_argument('--force', action='store_true', help='–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ —Ä–∞–∑–º–µ—Ä–µ')
    args = parser.parse_args()
    
    if not args.confirm:
        print("‚ùå –¢—Ä–µ–±—É–µ—Ç—Å—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: --confirm")
        print("   –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –∑–∞–º–µ–Ω–∏—Ç –¥–∞–Ω–Ω—ã–µ –≤ —É–∫–∞–∑–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö.")
        sys.exit(1)
    
    sheets_to_migrate = [s.strip() for s in args.sheets.split(',')]
    log.info(f"üìã –ú–∏–≥—Ä–∞—Ü–∏—è –ª–∏—Å—Ç–æ–≤: {sheets_to_migrate}")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    sources = load_sources_config()
    extractor = GSheetsExtractor()
    
    # –ù–∞—Ö–æ–¥–∏–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è —É–∫–∞–∑–∞–Ω–Ω—ã—Ö –ª–∏—Å—Ç–æ–≤
    results = []
    for spreadsheet_id, config in sources.items():
        if not isinstance(config, dict) or 'sheets' not in config:
            continue
        for sheet in config['sheets']:
            sheet_id = sheet.get('id', '')
            if sheet_id in sheets_to_migrate or sheet.get('target_table', '').endswith(tuple(sheets_to_migrate)):
                sheet['spreadsheet_id'] = spreadsheet_id
                success = await migrate_table(sheet, extractor, force=args.force)
                results.append((sheet_id, success))
    
    # –ò—Ç–æ–≥–∏
    log.info("=" * 50)
    log.info("–ò–¢–û–ì–ò –ú–ò–ì–†–ê–¶–ò–ò:")
    for sheet_id, success in results:
        status = "‚úÖ" if success else "‚ùå"
        log.info(f"  {status} {sheet_id}")
    
    await DBConnection.close()

if __name__ == "__main__":
    asyncio.run(main())
